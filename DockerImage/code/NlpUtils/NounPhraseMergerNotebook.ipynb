{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c4b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import unicode_literals\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f1b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNoun(token):\n",
    "\treturn token.pos in [spacy.symbols.NOUN, spacy.symbols.PROPN, spacy.symbols.PRON]\n",
    "\n",
    "# mergeNounPhrasesDoc should be called before this...\n",
    "def getNounPhrases(sentence):\n",
    "\treturn [tok for tok in sentence if isNoun(tok)]\n",
    "\n",
    "def getLemma(nphrase):\n",
    "\tdef getLemmaInt(tok):\n",
    "\t\treturn tok.text.lower() if tok.lemma_ == u'-PRON-' else tok.lemma_\n",
    "\n",
    "\t##########################################\n",
    "\tdef getLemmaSkipDetsAndPron(nphrase):\n",
    "\t\treturn u' '.join([ getLemmaInt(w) for w in nphrase if w.pos != spacy.symbols.DET ])\n",
    "\n",
    "\t##########################################\n",
    "\ttext = getLemmaInt(nphrase[0]) if len(nphrase) <= 1 else getLemmaSkipDetsAndPron(nphrase)\n",
    "\ttext = re.sub(r'\\s+-\\s+', u'-', text)\n",
    "\ttext = re.sub(u'\\s+(\\'|\\u2019)s', u'\\'s', text)\n",
    "\treturn text\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "#FIXME definitely a better way to do this based on dep labels...\n",
    "# E.g., information about a user <--- data\n",
    "def getEntType(np):\n",
    "\tcontainOrgOrPers = False\n",
    "\tents = []\n",
    "\tfor tok in np:\n",
    "\t\tif tok.ent_type_ in [u'ORG', u'PERSON']:\n",
    "\t\t\tcontainOrgOrPers = True\n",
    "\t\tif tok.ent_type_ == u'DATA':\n",
    "\t\t\treturn u'DATA'\n",
    "\t\tif tok.ent_type_ is not None and len(tok.ent_type_) > 0:\n",
    "\t\t\tents.append(tok.ent_type_)\n",
    "\tif containOrgOrPers:\n",
    "\t\treturn u'ORG'\n",
    "\treturn most_common(ents) if len(ents) > 0 else u''\n",
    "\n",
    "def mergeNounPhrasesDoc(doc, vocab, skipHeadWords=False):\n",
    "\tdef mergeNounPhrasesInternal(nphrases, fixDeps=False):\n",
    "\t\twith doc.retokenize() as retokenizer:\n",
    "\t\t\tfor np in nphrases:\n",
    "\t\t\t\t# This messes up the hearst pattterns, so don't merge!!!\n",
    "\t\t\t\tif skipHeadWords and len(np) > 1 and np[0].lemma_ in [u'other', u'such', u'especially']: \n",
    "\t\t\t\t\tnp = doc[np[0].i + 1 : np[-1].i + 1]\n",
    "\t\t\t\tif fixDeps:\n",
    "\t\t\t\t\tattrs = {\"pos\":spacy.symbols.NOUN, \"lemma\":getLemma(np), \"dep\":np[-1].dep, \"ent_type\":getEntType(np)}\n",
    "\t\t\t\t\tretokenizer.merge(np, attrs=attrs)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tattrs = {\"pos\":spacy.symbols.NOUN, \"lemma\":getLemma(np), \"ent_type\":getEntType(np)}\n",
    "\t\t\t\t\tretokenizer.merge(np, attrs=attrs)\n",
    "\t\t\t\t\t\n",
    "\t##########################################\n",
    "\tdef extractNPsWithAdPositions(doc):\n",
    "\t\tdef getPobj(token):\n",
    "\t\t\tpobjs = [ child for child in token.children if child.dep == spacy.symbols.pobj ]\n",
    "\t\t\treturn pobjs[0] if len(pobjs) > 0 else None\n",
    "\t\t##########################################\n",
    "\t\tdef getPobjsEndIndex(token, startIndex):\n",
    "\t\t\tif isNoun(token):\n",
    "\t\t\t\tfor ctok in token.children:\n",
    "\t\t\t\t\tif ctok.dep == spacy.symbols.prep and ctok.lemma_ in [u'of', u'on', u'in', u'about', u'regard']:\n",
    "\t\t\t\t\t\tpobj = getPobj(ctok)\n",
    "\t\t\t\t\t\tif pobj is not None:\n",
    "\t\t\t\t\t\t\t# Ensure that no space...\n",
    "\t\t\t\t\t\t\tif ctok.i != token.i + 1 or pobj.i != ctok.i + 1: # Ensure consecutive for now...\n",
    "\t\t\t\t\t\t\t\t#print 'PROBLEM: ', doc[token.i : pobj.i + 1]\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t#print 'PRINT: ', doc[token.i : pobj.i + 1], token.i, pobj.i, ctok.i\n",
    "\t\t\t\t\t\t\t\treturn getPobjsEndIndex(pobj, pobj.i)\n",
    "\t\t\treturn startIndex\n",
    "\t\t##########################################\n",
    "\t\tnphrases = []\n",
    "\t\tfor sentence in doc.sents:\n",
    "\t\t\tindex = 0\n",
    "\t\t\twhile index < len(sentence):\n",
    "\t\t\t\ttoken = sentence[index]\n",
    "\t\t\t\tendIndex = getPobjsEndIndex(token, -1)\n",
    "\t\t\t\tif endIndex > 0:\n",
    "\t\t\t\t\tnphrases.append(doc[token.i : endIndex + 1])\n",
    "\t\t\t\t\tindex = endIndex + 1 # Continue where we left off...\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tindex += 1\n",
    "\t\treturn nphrases\n",
    "\n",
    "\t##########################################\n",
    "\t# spaCy messes up extracing noun phrases when \"personally\" appears at the\n",
    "\t# beginning of the sentence (e.g., \"{Personally} {identifiable information} may\n",
    "\t# be shared\")\n",
    "\tdef fixSpacyNPhrase(doc, vocab):\n",
    "\t\tresults = []\n",
    "\t\t##########################################\n",
    "\t\tdef callback(matcher, doc, i, matches):\n",
    "\t\t\tfor match_id, start, end in matches:\n",
    "\t\t\t\tspan = doc[start:end]\n",
    "\t\t\t\tresults.append(span)\n",
    "\t\t##########################################\n",
    "\t\tmatcher = Matcher(vocab)\n",
    "\t\tmatcher.add(1,\n",
    "\t\t\t[[\t{spacy.attrs.LOWER: \"personally\"},\n",
    "\t\t\t\t{spacy.attrs.LOWER: \"identifiable information\"},\n",
    "\t\t\t]],on_match=callback)\n",
    "\t\tmatcher.add(2,\n",
    "\t\t\t[[\t{spacy.attrs.LOWER: \"personally\"},\n",
    "\t\t\t\t{spacy.attrs.LOWER: \"identifiable data\"},\n",
    "\t\t\t]],on_match=callback)\n",
    "\t\tmatcher.add(3,\n",
    "\t\t\t[[\t{spacy.attrs.LOWER: \"mobile device\"},\n",
    "\t\t\t\t{spacy.attrs.LOWER: \"identifier\"},\n",
    "\t\t\t]],on_match=callback)\n",
    "\t\tmatcher.add(4,\n",
    "\t\t\t[[\t{spacy.attrs.LOWER: \"device\"},\n",
    "\t\t\t\t{spacy.attrs.LOWER: \"identifier\"},\n",
    "\t\t\t]],on_match=callback)\n",
    "\t\t\n",
    "\t\t# X, credit or debit card information and other Y messes up, so let's make sure we merge these...\n",
    "\t\t# Disable this because it was caushing a crash to occur...\n",
    "#\t\tmatcher.add(5,callback,\n",
    "#\t\t\t[\t{spacy.attrs.LOWER: \"credit\"},\n",
    "#\t\t\t\t{spacy.attrs.POS: \"CCONJ\"},\n",
    "#\t\t\t\t{spacy.attrs.POS: \"NOUN\",  'OP': '+'}\n",
    "#\t\t\t])\n",
    "#\t\tmatcher.add(6,callback,\n",
    "#\t\t\t[\t{spacy.attrs.LOWER: \"debit\"},\n",
    "#\t\t\t\t{spacy.attrs.POS: \"CCONJ\"},\n",
    "#\t\t\t\t{spacy.attrs.POS: \"NOUN\",  'OP': '+'}\n",
    "#\t\t\t])\n",
    "\n",
    "\t\tmatcher(doc)\n",
    "\t\tmergeNounPhrasesInternal(results, fixDeps=True) #Ensure dependency label is set as \"identifiable information\" and not \"personally\"...\n",
    "\t##########################################\n",
    "\n",
    "\t# Gets relative clauses for BROAD or non-specific information types (information that..., certain information, ...)\n",
    "\tdef getRelativeClauses(doc):\n",
    "\t\tdef getSubject(token):\n",
    "\t\t\tsubjs = [ t for t in token.children if t.dep in [spacy.symbols.nsubj, spacy.symbols.nsubjpass] ]\n",
    "\t\t\treturn subjs[0] if len(subjs) > 0 else None\n",
    "\t\t##########################################\n",
    "\n",
    "\t\tdef getDirectObject(token):\n",
    "\t\t\tdobjs = [ t for t in token.children if t.dep == spacy.symbols.dobj ]\n",
    "\t\t\treturn dobjs[0] if len(dobjs) > 0 else None\n",
    "\t\t##########################################\n",
    "\n",
    "\t\tdef getRelclEndIndex(token, endIndex):\n",
    "\t\t\tif isNoun(token):\n",
    "\t\t\t\tfor child in token.children:\n",
    "\t\t\t\t\t# Note spacy.symbols.relcl is not defined...\n",
    "\t\t\t\t\tif child.dep_ == u'relcl': #TODO check POS to ensure verb?\n",
    "\t\t\t\t\t\tsubj = getSubject(child)\n",
    "\t\t\t\t\t\tif subj is not None and subj.lemma_ in [u'that', u'which']: #Do we ever really have multiple subjects?\n",
    "\t\t\t\t\t\t\tdobjs = getDirectObject(child)\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tif dobjs is not None:\n",
    "\t\t\t\t\t\t\t\tif subj.i != token.i + 1 and dobjs.i != subj.i + 1:\n",
    "\t\t\t\t\t\t\t\t\t#print 'PROBLEM3: ', doc[token.i : dobjs.i + 1]\n",
    "\t\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\treturn getRelclEndIndex(dobjs, dobjs.i)\n",
    "\t\t\treturn endIndex\n",
    "\t\t##########################################\n",
    "\n",
    "\t\tmergeList = []\n",
    "\t\tfor sentence in doc.sents:\n",
    "\t\t\tfor tok in sentence:\n",
    "\t\t\t\tendIndex = getRelclEndIndex(tok, -1)\n",
    "\t\t\t\tif endIndex > 0:\n",
    "\t\t\t\t\tif re.search(r'^((any|other|aggregate(d)?|various|the\\s(type|kind)(s)?\\sof|(small|large|wide|average)\\s(amount|range)\\sof|certain|specific)\\s+)?(information|data|datum|anything)(\\s+|$)', tok.lemma_):\n",
    "\t\t\t\t\t\t#print 'INCLUDE:', doc[tok.i : endIndex + 1]\n",
    "\t\t\t\t\t\tmergeList.append(doc[tok.i : endIndex + 1])\n",
    "\t\t\t#\t\telif re.search(r'(^|\\s*)(information|datum|anything)(\\s+|$)', tok.lemma_):\n",
    "\t\t\t#\t\t\tprint 'EXCLUDE:', doc[tok.i : endIndex + 1]\n",
    "\t\treturn mergeList\n",
    "\n",
    "\t##########################################\n",
    "\tdef spanContainsEgIe(span):\n",
    "\t\tfor tok in span:\n",
    "\t\t\tif tok.text in [u'i.e.', u'e.g.']:\n",
    "\t\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "\t##########################################\n",
    "\n",
    "\t# Merge entities...\n",
    "\twith doc.retokenize() as retokenizer:\n",
    "\t\tfor e in doc.ents:\n",
    "\t\t\t#Let's make sure that it doesn't start with a verb...\n",
    "\t\t\tif e[0].pos == spacy.symbols.VERB:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tattrs = {\"lemma\":getLemma(e)}    \n",
    "\t\t\tretokenizer.merge(e,attrs=attrs)\n",
    "\t\n",
    "\t#spacy.tokens.Token.set_extension('is_country', default=False)\n",
    "\t# Merge first using spaCy's default\n",
    "\tnphrases = [ nchunk for nchunk in doc.noun_chunks if not spanContainsEgIe(nchunk) ]\n",
    "\tmergeNounPhrasesInternal(nphrases)\n",
    "\n",
    "\t# Fix problems with spaCy mistagging \"personally\" when it occurs at the beginning\n",
    "\tfixSpacyNPhrase(doc, vocab)\n",
    "\n",
    "\t# Now loop back through and merge using our extended approach!\n",
    "\tnphrases = extractNPsWithAdPositions(doc)\n",
    "\tmergeNounPhrasesInternal(nphrases)\n",
    "\n",
    "\tnphrases = getRelativeClauses(doc)\n",
    "\tmergeNounPhrasesInternal(nphrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da8b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bede6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-policheckbert",
   "language": "python",
   "name": "venv-policheckbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
