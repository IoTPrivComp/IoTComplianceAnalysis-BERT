{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad321ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from TermPreprocessor2Notebook.ipynb\n",
      "importing Jupyter notebook from ConsistencyNotebook.ipynb\n",
      "importing Jupyter notebook from ConsistencyDatabaseNotebook.ipynb\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import unicode_literals\n",
    "import sys\n",
    "import csv\n",
    "import dill as pickle\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import TermPreprocessor2Notebook as tprep\n",
    "import spacy\n",
    "import ConsistencyNotebook as con\n",
    "\n",
    "import ConsistencyDatabaseNotebook as conDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5684d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DOCKER = True \n",
    "#ROOT_DIR='/ext/' if USE_DOCKER else '../../ext/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21033e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixEntityLemma(txt, nlp):\n",
    "\tdef getLemma(tok):\n",
    "\t\treturn tok.lemma_ if tok.lemma_ != u'-PRON-' else tok.text\n",
    "\t#txt=txt.decode('utf-8')\n",
    "\t#print(txt, 'txt\\n')\n",
    "\tdoc = nlp(txt)\n",
    "\t#print(doc, 'doc\\n')\n",
    "\treturn ' '.join([getLemma(t) for t in doc if t.pos != spacy.symbols.DET ])\n",
    "\n",
    "def LOG_ERROR(outputfilename, message, outputDir='../../ext/output/log_data'):\n",
    "\toutputDir = '../../ext/output/log_data'#os.path.join(ROOT_DIR, outputDir)\n",
    "\twith codecs.open(os.path.join(outputDir, outputfilename), 'a', 'utf-8') as logfile:\n",
    "\t\tlogfile.write(message)\n",
    "\t\tlogfile.write('\\n')\n",
    "\n",
    "def ensureUnicode(s):\n",
    "\treturn s if type(s) == str else s\n",
    "\n",
    "\n",
    "def loadFlowResults(filename, packageName, cdb, subsetNum):\n",
    "\tcolumn_names = ['package_name', 'app_name', 'version_name', 'version_code', 'data_type', 'dest_domain', 'dest_ip', 'arb_number', 'privacy_policy']\n",
    "\n",
    "\tevents_df = pd.read_csv(filename, header = 0, names = column_names, dtype={'app_name': str, 'privacy_policy': str})\n",
    "\tevents_df.fillna(u'', inplace=True)\n",
    "\n",
    "\tdataMap = {\n",
    "\t\tu'aaid' : u'advertising id',\n",
    "\t\tu'fingerprint' : None,\n",
    "\t\tu'androidid' : u'android id',\n",
    "\t\tu'hwid' : u'serial number',\n",
    "\t\tu'routerssid' : u'router ssid',\n",
    "\t\tu'routermac' : u'mac address',\n",
    "\t\tu'imei' : u'imei',\n",
    "\t\tu'wifimac' : u'mac address',\n",
    "\t\tu'invasive' : None,\n",
    "\t\tu'package_dump' : u'application instal',\n",
    "\t\tu'real_name' : u'person name',\n",
    "\t\tu'hlthwel' : u'wellness-relate datum',\n",
    "\t\tu'ftnut' : u'fitness and nutrition-relate information',\n",
    "\t\tu'gsfid' : u'gsfid',\n",
    "\t\tu'audio' : u'audio',\n",
    "\t\tu'calendar' : u'calendar',\n",
    "\t\tu'camera' : u'camera',\n",
    "\t\tu'sound' : u'sound',\n",
    "\t\tu'email' : u'email address',\n",
    "\t\tu'gallery' : u'photo',\n",
    "\t\tu'geolocation' : u'geographical location',\n",
    "\t\tu'health_wellness' : u'health and wellness',\n",
    "\t\tu'motion' : u'motion',\n",
    "\t\tu'nfc' : u'nfc',\n",
    "\t\tu'payment' : u'payment',\n",
    "\t\tu'phone' : u'phone',\n",
    "\t\tu'simid' : u'sim serial number',\n",
    "\t\tu'sms' : u'text message',\n",
    "\t\tu'socialmedia_activity' : u'social media information',\n",
    "\t\tu'video' : u'video',\n",
    "\t\tu'voice' : u'voice',\n",
    "\t\tu'music' : u'music',\n",
    "\t\tu'router' : u'router',\n",
    "\t\tu'weather' : u'weather'\n",
    "\t}\n",
    "    \n",
    "    \n",
    "    \n",
    "\tpNameNoVers = re.sub(r'-[0-9]+$', '', packageName)\n",
    "\n",
    "\tflows = []\n",
    "\tfor _,package_name, app_name, version_name, version_code, data_type, dest_domain, dest_ip, arb_number, privacy_policy in events_df.itertuples():\n",
    "\t\tif package_name != pNameNoVers:\n",
    "\t\t\tcontinue\n",
    "\t\tif dest_domain == '':\n",
    "\t\t\tdest_domain = dest_ip\n",
    "\t\t#TODO check if URL is first party or second party\n",
    "\t\tresolvedEntity = tprep.resolveUrl(dest_domain, package_name, privacy_policy)\n",
    "\t\tresolvedData = dataMap[data_type]\n",
    "\n",
    "\t\tif resolvedData is None:\n",
    "\t\t\t# log and continue\n",
    "\t\t\tLOG_ERROR('SkippedDataFlows_{}.log'.format(subsetNum), '{},{}'.format(data_type,packageName))\n",
    "\t\t\tprint(\"skipped dataflows\") # 0 count\n",
    "\t\t\tcontinue\n",
    "\t\tif resolvedEntity not in con.Entity.ontology.nodes:#TODO log\n",
    "\t\t\tLOG_ERROR('SkippedEntityFlows_{}.log'.format(subsetNum), '{},{}'.format(dest_domain,packageName))\n",
    "\t\t\tprint(\"resolved entity not there: \", resolvedEntity) # 63 count (java.util.map - ok)          \n",
    "\t\t\tcontinue\n",
    "\t\tif resolvedData not in con.DataObject.ontology.nodes:\n",
    "\t\t\tLOG_ERROR('SkippedDataFlowsOnt_{}.log'.format(subsetNum), '{},{}'.format(data_type,packageName))\n",
    "\t\t\tprint(\"skipped dataflowont\") # 0 count\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# TODO Don't put a duplicates...\n",
    "\t\tdflow = con.DataFlow((resolvedEntity, resolvedData))\n",
    "\t\tif dflow not in flows:\n",
    "\t\t\tflows.append(dflow)\n",
    "\t\t\tcdb.insertDataFlow(resolvedEntity, resolvedData)\n",
    "\n",
    "\t\tcdb.insertAppDataFlow(packageName, resolvedEntity, resolvedData, dest_domain, data_type)\n",
    "\tprint ('\\tLoaded {} flows for {}'.format(len(flows), pNameNoVers))\n",
    "\treturn flows\n",
    "\n",
    "\n",
    "def shouldIgnoreSentence(s):\n",
    "\tmentionsChildRegex = re.compile(r'\\b(child(ren)?|kids|from\\sminor(s)?|under\\s1[0-9]+|under\\s(thirteen|fourteen|fifteen|sixteen|seventeen|eighteen)|age(s)?(\\sof)?\\s1[0-9]+|age(s)?(\\sof)?\\s(thirteen|fourteen|fifteen|sixteen|seventeen|eighteen))\\b', flags=re.IGNORECASE)\n",
    "\tmentionsUserChoiceRegex = re.compile(r'\\b(you|user)\\s(.*\\s)?(choose|do|decide|prefer)\\s.*\\s(provide|send|share|disclose)\\b', flags=re.IGNORECASE)\n",
    "\tmentionsUserChoiceRegex2 = re.compile(r'\\b((your\\schoice)|(you\\sdo\\snot\\shave\\sto\\sgive))\\b', flags=re.IGNORECASE)\n",
    "\t# TODO remove false positives that discuss \"except as discussed in this privacy policy / below\"\n",
    "#5\tmentionsExceptInPrivacyPol1 = re.compile(r'\\b(except\\sas\\s(stated|described|noted))\\b', flags=re.IGNORECASE)\n",
    "#6\tmentionsExceptInPrivacyPol2 = re.compile(r'\\b(except\\sin(\\sthose\\slimited)?\\s(cases))\\b', flags=re.IGNORECASE)\n",
    "\n",
    "#3\tif mentionsChildRegex.search(s) or mentionsUserChoiceRegex.search(s) or mentionsUserChoiceRegex2.search(s) or mentionsExceptInPrivacyPol1.search(s) or mentionsExceptInPrivacyPol2.search(s):\n",
    "\tif mentionsChildRegex.search(s) or mentionsUserChoiceRegex.search(s) or mentionsUserChoiceRegex2.search(s):        \n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "\n",
    "def loadPrivacyPolicyResults(filename, packageName, cdb, nlp, subsetNum):\n",
    "\tif not os.path.isfile(filename):\n",
    "\t\treturn []\n",
    "\tpolicy = []\n",
    "\tfor e,c,d,s,aLemma in pickle.load(open(filename, 'rb')):\n",
    "\t\te = ensureUnicode(e)\n",
    "\t\tc = ensureUnicode(c)\n",
    "\t\td = ensureUnicode(d)\n",
    "\t\ts = ensureUnicode(s)\n",
    "\n",
    "\t\tif c == 'not_collect' and shouldIgnoreSentence(s):\n",
    "\t\t\tprint(\"should ignore sentence\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\teproc = tprep.preprocess(fixEntityLemma(e, nlp))\n",
    "\t\tif eproc in [u'user', u'you', u'person', u'consumer', u'participant']:\n",
    "\t\t\tprint(\"eproc in user, you, etc.\", eproc) # 10 count\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t#TODO Should we try to resolve company name or ignore entity all together?\n",
    "\t\t# u'we_implicit'\n",
    "\t\tif eproc in [u'we', u'i', u'us', u'me'] or eproc in [u'app', u'mobile application', u'mobile app', u'application', u'service', u'website', u'web site', u'site'] or (e.startswith('our') and eproc in [u'app', u'mobile application', u'mobile app', u'application', u'service', u'company', u'business', u'web site', u'website', u'site']):\n",
    "\t\t\teproc = u'we'\n",
    "\n",
    "#4\t\tif eproc == u'third_party_implicit' or eproc == u'we_implicit' or eproc == u'anyone':\n",
    "#\t\t\tprint(\"e proc in third_party_implict, we_implicit, etc.\")\n",
    "#\t\t\tcontinue\n",
    "\t\tif eproc == u'third_party_implicit' or eproc == u'anyone':\n",
    "\t\t\teproc = u'third-party'\n",
    "\t\tif eproc == u'we_implicit':\n",
    "\t\t\teproc = u'we'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\t\tif eproc == u'third_party_implicit':\n",
    "#\t\t\teproc = u'third party'\n",
    "\t\tdproc = tprep.preprocess(d)\n",
    "\n",
    "\t\tents = []\n",
    "\t\tif eproc not in con.Entity.ontology.nodes:\n",
    "\t\t\tres = re.sub(r'\\b(and|or|and/or|\\/|&)\\b', u'\\n', eproc)\n",
    "\t\t\tfoundAnEnt = False\n",
    "\t\t\tfor e in res.split('\\n'):\n",
    "\t\t\t\te = e.strip()\n",
    "\t\t\t\tif e == u'third_party_implicit' or e == u'we_implicit' or e == u'anyone':\n",
    "\t\t\t\t\tprint(\"continue 1\") # 0 count\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tif e not in con.Entity.ontology.nodes:#This should really never happen...\n",
    "\t\t\t\t\tLOG_ERROR('/ext/SkippedPolicyEntities_{}.log'.format(subsetNum), e)\n",
    "\t\t\t\t\tprint(\"continue 2: \", e) # 1,075 count\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tents.append(e)\n",
    "\t\telse:\n",
    "\t\t\tents = [eproc]\n",
    "\n",
    "\t\tif len(ents) == 0:\n",
    "\t\t\tLOG_ERROR('/ext/SkippedPolicyEntities_{}.log'.format(subsetNum), eproc)\n",
    "\t\t\tprint(\"continue 3: \", eproc) # many counts\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif dproc in con.DataObject.ontology.nodes:\n",
    "\t\t\tprint(\"dproc in con.DataObject.ontology.nodes\")\n",
    "\t\t\tif dproc in con.DataObject.ontology.nodes and dproc != con.DataObject.root:\n",
    "\t\t\t\tfor e in ents:\n",
    "\t\t\t\t\tcdb.insertPolicy(e, c, dproc)\n",
    "\t\t\t\t\tcdb.insertAppPolicySentence(s, (e, c, dproc), packageName)\n",
    "\t\t\t\t\tpolicy.append((e, c, dproc, s))\n",
    "\t\telse:\n",
    "\t\t\tres = re.sub(r'\\b(and|or|and/or|\\/|&)\\b', u'\\n', dproc)\n",
    "\t\t\tfor d in res.split('\\n'):\n",
    "\t\t\t\td = d.strip()\n",
    "\t\t\t\tif d not in con.DataObject.ontology.nodes or d == con.DataObject.root:#This should really never happen...\n",
    "\t\t\t\t\tLOG_ERROR('/ext/SkippedPolicyDataObjects_{}.log'.format(subsetNum), d)\n",
    "\t\t\t\t\tprint(\"continue 4: \", d) # many counts\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tfor e in ents:\n",
    "\t\t\t\t\tif e == con.Entity.root:\n",
    "\t\t\t\t\t\tprint(\"continue 5\") # 0 counts\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\tcdb.insertPolicy(e, c, d)\n",
    "\t\t\t\t\tcdb.insertAppPolicySentence(s, (e, c, d), packageName)\n",
    "\t\t\t\t\tpolicy.append((e, c, d, s))\n",
    "\n",
    "\treturn policy\n",
    "\n",
    "\n",
    "def getPackageName(policyFilename):\n",
    "\tfname,_ = os.path.splitext(os.path.basename(policyFilename))\n",
    "\treturn fname\t\n",
    "\n",
    "def doFilesExist(filelist):\n",
    "\treturn all(os.path.exists(f) for f in filelist)\n",
    "\n",
    "def main(argv):\n",
    "\tsubsetNum = 1 #argv[1]\n",
    "\t#Assumes a number as input...\n",
    "\tconsistency_database_path = '../../ext/output/db/consistency_results_{}.db'.format(subsetNum)#os.path.join(ROOT_DIR, 'output/db/consistency_results_{}.db'.format(subsetNum))\n",
    "\tinputDataFilename = '../../ext/datasets/{}.txt'.format(subsetNum)#os.path.join(ROOT_DIR, 'datasets/{}.txt'.format(subsetNum))\n",
    "\tprogressFilename = '../../ext/output/log_data/{}.log'.format(subsetNum)#os.path.join(ROOT_DIR, 'output/log_data/{}.log'.format(subsetNum))\n",
    "\n",
    "\tcdb = conDB.ConsistencyDB(consistency_database_path)#'consistency_results.db'\n",
    "\tcon.init(dataOntologyFilename=u'../../ext/data/data_ontology.pickle', entityOntologyFilename=u'../../ext/data/entity_ontology.pickle')\n",
    "#\tcon.init_static()\n",
    "\tnlp = spacy.load(\"en_core_web_trf\", exclude=[\"ner\"])#nlp = spacy.load('/ext/NlpFinalModel')\n",
    "\tnlp_entity = spacy.load(\"../../ext/model-best\")\n",
    "\tnlp.add_pipe(\"ner\", source=nlp_entity)\n",
    "\n",
    "\tcdb.createTables()\n",
    "\t# Let's walk the policy directory now...\n",
    "\tprogress_file = codecs.open(progressFilename, 'a', 'utf-8')\n",
    "\t\n",
    "\tfiles = [ line.strip() for line in codecs.open(inputDataFilename, 'r', 'utf-8') ]\n",
    "\tfor polPath in files:\n",
    "\t\tprint ('Starting', polPath)\n",
    "\t\tprogress_file.write('Starting {}\\n'.format(polPath))\n",
    "\t\tpackageName = getPackageName(polPath)\n",
    "\t\tpolicy = loadPrivacyPolicyResults(os.path.join('../../ext/output/policy/', polPath), packageName, cdb, nlp, subsetNum)\n",
    "\t\tpolicy = [con.PolicyStatement(p) for p in set(policy) ]\n",
    "\t\tflows = loadFlowResults('../../ext/data/flows.csv', packageName, cdb, subsetNum)\n",
    "\t\tprint ('\\tLoaded {} policy statements for {}'.format(len(policy), packageName))\n",
    "\n",
    "\n",
    "\t\t#PolicyLint Analysis...\n",
    "\t\tpolicyContradictions = con.getContradictions(policy, packageName)\n",
    "\t\tfor (p0, p1), contradictionIndex in policyContradictions:\n",
    "\t\t\tprint (p0,p1,contradictionIndex, packageName)\n",
    "\t\t\tprint (cdb.insertContradiction(contradictionIndex, packageName, p0.getTuple(), p1.getTuple()))\t\t\n",
    "\t\t\n",
    "\n",
    "\t\t#PoliCheck Analysis...\n",
    "\t\tif len(flows) == 0:\n",
    "\t\t\tLOG_ERROR('SkippedAppsNoFlows_{}.log'.format(subsetNum), packageName)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tconsistencyResults = con.checkConsistency(policy, flows)\n",
    "\t\tfor cres in consistencyResults:\n",
    "\t\t\tflow = cres['flow']\n",
    "\t\t\tisConsistent,policies,contradictions = cres['consistency']\n",
    "\n",
    "\t\t\tcdb.insertConsistencyResult(flow.entity.entity, flow.data.data, packageName, isConsistent)\n",
    "\n",
    "\t\t\tnumContradictions = 0\n",
    "\t\t\tif policies is not None:\n",
    "\t\t\t\tfor i,p in enumerate(policies):\n",
    "\t\t\t\t\tpTuple = (p.entity.entity, p.action.action, p.data.data)\n",
    "\t\t\t\t\tif contradictions is not None and contradictions[i] is not None:\n",
    "\t\t\t\t\t\tfor c,cnum in contradictions[i]:\n",
    "\t\t\t\t\t\t\tnumContradictions += 1\n",
    "\t\t\t\t\t\t\tcTuple = (c.entity.entity, c.action.action, c.data.data)\n",
    "\t\t\t\t\t\t\tcdb.insertConsistencyData(flow.entity.entity, flow.data.data, packageName, pTuple, cTuple, cnum)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcdb.insertConsistencyData(flow.entity.entity, flow.data.data, packageName, pTuple, None, -1)\n",
    "\n",
    "\t\t\tnumPolicies = len(policies) if policies is not None else 0\n",
    "\t\t\tprint ('\\tFlow: {}\\n\\t\\tIs Consistent: {}\\n\\t\\tNum Policies: {}\\n\\t\\tNum Contradictions: {}\\n'.format(flow, isConsistent, numPolicies, numContradictions))\n",
    "\t\n",
    "\t\tprint ('Ending', polPath)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain(sys.argv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-policheckbert",
   "language": "python",
   "name": "venv-policheckbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
